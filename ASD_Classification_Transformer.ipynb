{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZQhhvNJ1PHOcBbnHlGbEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-abbas-ansari/ASD-Classification/blob/main/ASD_Classification_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Setup and Dataset Download"
      ],
      "metadata": {
        "id": "8I2_51BFTAua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('./gdrive')"
      ],
      "metadata": {
        "id": "Ggh8nBCNXogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd3LL4_xSvPI"
      },
      "outputs": [],
      "source": [
        "!pip -q install wget wandb torchmetrics\n",
        "import wget\n",
        "wget.download(\"https://zenodo.org/record/2647418/files/TrainingDataset.rar?download=1\")\n",
        "!mkdir Dataset\n",
        "!unrar x TrainingDataset.rar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Utilities"
      ],
      "metadata": {
        "id": "_rFg663JUhu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import operator\n",
        "from glob import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.functional import accuracy, auc, precision_recall, f1_score"
      ],
      "metadata": {
        "id": "f8OWmsakS8rv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(anno_path):\n",
        "    anno_dict = dict()\n",
        "    max_len = dict()\n",
        "    # Saliency4ASD has 300 images\n",
        "    for i in range(1,301):\n",
        "        img = cv2.imread(os.path.join(anno_path,'Images',str(i)+'.png'))\n",
        "        y_lim, x_lim, _ = img.shape\n",
        "        anno_dict[i] = dict()\n",
        "        anno_dict[i]['img_size'] = [y_lim,x_lim]\n",
        "        asd = pd.read_csv(os.path.join(anno_path,'ASD','ASD_scanpath_'+str(i)+'.txt'))\n",
        "        ctrl = pd.read_csv(os.path.join(anno_path,'TD','TD_scanpath_'+str(i)+'.txt'))\n",
        "        group_name = ['ctrl','asd']\n",
        "        for flag, group in enumerate([ctrl, asd]):\n",
        "            anno_dict[i][group_name[flag]] = dict()\n",
        "            anno_dict[i][group_name[flag]]['fixation'] = []\n",
        "            anno_dict[i][group_name[flag]]['duration'] = []\n",
        "            cur_idx = list(group['Idx'])\n",
        "            cur_x = list(group[' x'])\n",
        "            cur_y = list(group[' y'])\n",
        "            cur_dur = list(group[' duration'])\n",
        "            tmp_fix = []\n",
        "            tmp_dur = []\n",
        "            for j in range(len(cur_idx)):\n",
        "                # finish loading data for one subject\n",
        "                if cur_idx[j] == 0  and j != 0:\n",
        "                    anno_dict[i][group_name[flag]]['fixation'].append(tmp_fix)\n",
        "                    anno_dict[i][group_name[flag]]['duration'].append(tmp_dur)\n",
        "                    tmp_fix = []\n",
        "                    tmp_dur = []\n",
        "                tmp_fix.append([cur_y[j],cur_x[j]])\n",
        "                tmp_dur.append(cur_dur[j])\n",
        "            # save data of the last subject\n",
        "            anno_dict[i][group_name[flag]]['fixation'].append(tmp_fix)\n",
        "            anno_dict[i][group_name[flag]]['duration'].append(tmp_dur)\n",
        "\n",
        "    return anno_dict"
      ],
      "metadata": {
        "id": "26z1J70dUoOj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(data.Dataset):\n",
        "    def __init__(self,img_dir,data,max_len,img_height,img_width,transform):\n",
        "        self.img_dir = img_dir\n",
        "        self.initial_dataset(data)\n",
        "        self.max_len = max_len\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.transform = transform\n",
        "\n",
        "    def initial_dataset(self,data):\n",
        "        self.fixation = []\n",
        "        self.duration = []\n",
        "        self.label = []\n",
        "        self.img_id = []\n",
        "        self.img_size = []\n",
        "\n",
        "        for img_id in data.keys():\n",
        "            # if not img_id in valid_id:\n",
        "            #     continue\n",
        "            for group_label, group in enumerate(['ctrl','asd']):\n",
        "                self.fixation.extend(data[img_id][group]['fixation'])\n",
        "                self.duration.extend(data[img_id][group]['duration'])\n",
        "                self.img_id.extend([os.path.join(self.img_dir,str(img_id)+'.png')]*len(data[img_id][group]['fixation']))\n",
        "                self.label.extend([group_label]*len(data[img_id][group]['fixation']))\n",
        "                self.img_size.extend([data[img_id]['img_size']]*len(data[img_id][group]['fixation']))\n",
        "\n",
        "    def get_fix_dur(self,idx):\n",
        "        fixs = self.fixation[idx]\n",
        "        durs = self.duration[idx]\n",
        "        y_lim, x_lim = self.img_size[idx]\n",
        "        fixation = []\n",
        "        duration = []\n",
        "        invalid = 0\n",
        "        # only consider the first k fixations\n",
        "        for i in range(self.max_len):\n",
        "            if i+1 <= len(fixs):\n",
        "                y_fix, x_fix = fixs[i]\n",
        "                dur = durs[i]\n",
        "                x_fix = int(x_fix*(self.img_width/float(x_lim))/32)\n",
        "                y_fix = int(y_fix*(self.img_height/float(y_lim))/33)\n",
        "                if x_fix >=0 and y_fix>=0:\n",
        "                    fixation.append(y_fix*25 + x_fix) # get the corresponding index of fixation on the downsampled feature map\n",
        "                    duration.append(dur) # duration of corresponding fixation\n",
        "                else:\n",
        "                    invalid += 1\n",
        "            else:\n",
        "                fixation.append(0) # pad if necessary\n",
        "                duration.append(0)\n",
        "        for i in range(invalid):\n",
        "            fixation.append(0)\n",
        "            duration.append(0)\n",
        "        fixation = torch.from_numpy(np.array(fixation).astype('int'))\n",
        "        duration = torch.from_numpy(np.array(duration).astype('int'))\n",
        "        return fixation, duration\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(self.img_id[index])\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = torch.FloatTensor([self.label[index]])\n",
        "        fixation, duration = self.get_fix_dur(index)\n",
        "        return img, label, fixation, duration\n",
        "\n",
        "    def __len__(self,):\n",
        "        return len(self.fixation)"
      ],
      "metadata": {
        "id": "iDX0XnN3UqF-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def simple_img_split(anno_dict, val_ratio):\n",
        "  num_imgs = len(anno_dict.keys())\n",
        "  print(num_imgs)\n",
        "  val_idx = list(random.sample(range(1, num_imgs+1), k=int(val_ratio*num_imgs))) # randomly select validation images\n",
        "  train_idx = list(set(val_idx) ^ set(range(1, num_imgs+1)))\n",
        "  train_dict = {k: anno_dict[k] for k in train_idx}\n",
        "  val_dict = {k: anno_dict[k] for k in val_idx}\n",
        "\n",
        "  return train_dict, val_dict\n",
        "\n",
        "os.makedirs('checkpoints', exist_ok=True)"
      ],
      "metadata": {
        "id": "b0Xtlf-bUr8C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Archtitecture"
      ],
      "metadata": {
        "id": "H5GOGWwvUwLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "def adjust_lr(optimizer, epoch):\n",
        "    \"adatively adjust lr based on epoch\"\n",
        "    if epoch <= 0 :\n",
        "        lr = LR\n",
        "    else :\n",
        "        lr = LR * (0.5 ** (float(epoch) / 2))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "MeMGc45aUt5m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute sinusoid encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \"\"\"\n",
        "        constructor of sinusoid encoding class\n",
        "\n",
        "        :param d_model: dimension of model\n",
        "        :param max_len: max sequence length\n",
        "        :param device: hardware device setting\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # same size with input matrix (for adding with input matrix)\n",
        "        self.encoding = torch.zeros(max_len, d_model).cuda()\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len).cuda()\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        # 1D => 2D unsqueeze to represent word's position\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2).float().cuda()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model))).cuda()\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model))).cuda()\n",
        "        #print(f'shape of encoding = {self.encoding.size()}')\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, d_model = x.size()\n",
        "        return self.encoding.expand(batch, seq_len, d_model) "
      ],
      "metadata": {
        "id": "0mNb2ZIZUzdX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
        "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
        "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "        # with every other training example, don't be confused by einsum\n",
        "        # it's just how I like doing matrix multiplication & bmm\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        # queries shape: (N, query_len, heads, heads_dim),\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy: (N, heads, query_len, key_len)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "        # attention shape: (N, heads, query_len, key_len)\n",
        "        # values shape: (N, value_len, heads, heads_dim)\n",
        "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        # Add skip connection, run through normalization and finally dropout\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_size,\n",
        "        num_layers,\n",
        "        heads,\n",
        "        device,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "    ):\n",
        "        print(f'Got parameters: embed_size = {embed_size} num_layers = {num_layers} heads = {heads} device = {device} forward_expansion = {forward_expansion}\\\n",
        "        dropout = {dropout} max_lenght = {max_length}')\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.position_embedding = PositionalEncoding(embed_size, max_length, device)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    embed_size,\n",
        "                    heads,\n",
        "                    dropout=dropout,\n",
        "                    forward_expansion=forward_expansion,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_length, emb_dim = x.shape\n",
        "        out = self.dropout(\n",
        "            (x+ self.position_embedding(x))\n",
        "        )\n",
        "\n",
        "        # In the Encoder the query, key, value are all the same, it's in the\n",
        "        # decoder this will change. This might look a bit odd in this case.\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "tW1ZA4juU5OT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sal_transformer(nn.Module):\n",
        "  def __init__(self, backend, seq_len, num_blocks, heads, device, expansion, drop_prob, emb_dim):\n",
        "      super(Sal_transformer,self).__init__()\n",
        "      print(f'Got parameters: backend = {backend} num_blocks = {num_blocks} heads = {heads} device = {device} expansion = {expansion}\\\n",
        "        drop_prob = {drop_prob} emb_dim = {emb_dim}')\n",
        "      self.seq_len = seq_len\n",
        "      self.emb_dim = emb_dim\n",
        "      # defining backend\n",
        "      if backend == 'resnet':\n",
        "          resnet = models.resnet50(pretrained=True)\n",
        "          self.init_resnet(resnet)\n",
        "          self.input_size = 2048\n",
        "      elif backend == 'vgg':\n",
        "          vgg = models.vgg19(pretrained=True)\n",
        "          self.init_vgg(vgg)\n",
        "          self.input_size = 512\n",
        "      else:\n",
        "          assert 0, f\"Backend '{backend}' not implemented\"\n",
        "      if self.input_size != emb_dim:\n",
        "        self.project = nn.Linear(self.input_size, emb_dim)\n",
        "      self.transformer = Encoder(emb_dim, num_blocks, heads, device, expansion, drop_prob, seq_len)\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(emb_dim, emb_dim//4),\n",
        "          nn.Linear(emb_dim//4, emb_dim//8),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear((emb_dim//8)*seq_len, 1))\n",
        "\n",
        "  def init_resnet(self,resnet):\n",
        "      self.backend = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "  def init_vgg(self,vgg):\n",
        "      # self.backend = vgg.features\n",
        "      self.backend = nn.Sequential(*list(vgg.features.children())[:-2]) # omitting the last Max Pooling\n",
        "\n",
        "  def forward(self,x,fixation, duration):\n",
        "      x = self.backend(x) # [12, 2048, 19, 25]\n",
        "      batch, feat, h, w = x.size()\n",
        "      x = x.view(batch,feat,-1) # [12, 2048, 475]\n",
        "      \n",
        "      # recurrent loop\n",
        "      fixation = fixation.view(fixation.size(0),1,fixation.size(1)) # [12, 1, 14]\n",
        "      fixation = fixation.expand(fixation.size(0),feat,fixation.size(2)) # [12, 2048, 14]\n",
        "      #print(f'x before gather: {x}')\n",
        "      x = x.gather(2,fixation).transpose(1,2)\n",
        "      if self.input_size != self.emb_dim:\n",
        "        x = self.project(x)\n",
        "      output = self.transformer(x)\n",
        "\n",
        "      output = torch.sigmoid(self.decoder(output))\n",
        "      return output"
      ],
      "metadata": {
        "id": "N7gCWdBYZh7D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "SVzUWSvKVXjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-4\n",
        "img_dir = 'Dataset/TrainingData/Images'\n",
        "anno_dir = 'Dataset/TrainingData'\n",
        "backend = 'resnet'\n",
        "checkpoint_path= 'checkpoints'\n",
        "num_epochs = 10\n",
        "val_ratio = 0.1\n",
        "batch_size = 12\n",
        "max_len = 14\n",
        "clip = 10\n",
        "img_height = 600\n",
        "img_width = 800\n",
        "\n",
        "#Sal_transformer\n",
        "num_blocks=1 \n",
        "heads=4\n",
        "device='gpu'\n",
        "expansion=4\n",
        "drop_prob=0.4\n",
        "emb_dim=2048"
      ],
      "metadata": {
        "id": "7yRweKHGVJSC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Model Intialization"
      ],
      "metadata": {
        "id": "oIQIi6wSWJg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anno = read_dataset(anno_dir)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((img_height,img_width)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data, val_data = simple_img_split(anno, val_ratio)\n",
        "\n",
        "train_set = Dataset(img_dir, train_data, max_len, img_height, img_width, transform)\n",
        "val_set = Dataset(img_dir, val_data, max_len, img_height, img_width, transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUf_JauhVr6I",
        "outputId": "b07de0af-427e-4894-927a-d9c6c64624a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sal_transformer(backend, \n",
        "                        max_len, \n",
        "                        num_blocks, \n",
        "                        heads, \n",
        "                        device, \n",
        "                        expansion, \n",
        "                        drop_prob,\n",
        "                        emb_dim)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-5) "
      ],
      "metadata": {
        "id": "aAdIoBwjWRRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Total parameters in model: {total_params:,}\")"
      ],
      "metadata": {
        "id": "u5H4KIiqWYTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Validation Loop"
      ],
      "metadata": {
        "id": "7ZxGLKstXJoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(iteration):\n",
        "    avg_loss = 0\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for j, (img,target,fix, dur) in enumerate(tqdm(trainloader)):\n",
        "        if len(img) < batch_size:\n",
        "            continue\n",
        "        img, target, fix, dur = Variable(img), Variable(target.type(torch.FloatTensor)), Variable(fix,requires_grad=False), Variable(dur.type(torch.FloatTensor), requires_grad=False)\n",
        "        if torch.cuda.is_available():\n",
        "          img, target, fix, dur = img.cuda(), target.cuda(), fix.cuda(), dur.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img,fix, dur)\n",
        "        loss = F.binary_cross_entropy(pred,target)\n",
        "        loss.backward()\n",
        "        if clip != -1:\n",
        "            clip_gradient(optimizer,clip)\n",
        "        optimizer.step()\n",
        "        avg_loss = (avg_loss*np.maximum(0,j) + loss.data.cpu().numpy())/(j+1)\n",
        "\n",
        "        if j%25 == 0:\n",
        "            wandb.log({'bce loss': avg_loss}, step=iteration)\n",
        "        iteration += 1\n",
        "\n",
        "        preds.append(pred.cpu())\n",
        "        targets.append(target.to(torch.int16).cpu())\n",
        "    with torch.no_grad():\n",
        "      preds = torch.cat(preds, 0)\n",
        "      targets = torch.cat(targets, 0)\n",
        "      acc = accuracy(preds, targets)\n",
        "      auc_v = auc(preds, targets, reorder=True)\n",
        "      pre, rec = precision_recall(preds, targets)\n",
        "      score = f1_score(preds, targets)\n",
        "      print(f'\\nT {epoch}: acc = {acc.item():.2f} auc = {auc_v.item():.2f} pre = {pre.item():.2f} rec = {rec.item():.2f} f1_score = {score.item():.2f}')\n",
        "\n",
        "    return iteration"
      ],
      "metadata": {
        "id": "cvde6T1UW6LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch):\n",
        "  \n",
        "  preds = []\n",
        "  targets = []\n",
        "  with torch.no_grad():\n",
        "    for _, (img,target,fix, dur) in enumerate(valloader):\n",
        "      img, target, fix, dur = Variable(img), Variable(target.type(torch.FloatTensor)), Variable(fix,requires_grad=False), Variable(dur.type(torch.FloatTensor), requires_grad=False)\n",
        "      img, fix, dur = img.cuda(), fix.cuda(), dur.cuda()\n",
        "      #print(f'img: {img.shape} target: {target} fix: {fix.shape}')\n",
        "      # break\n",
        "      pred = model(img,fix, dur)\n",
        "      preds.append(pred.cpu())\n",
        "      targets.append(target.to(torch.int16))\n",
        "\n",
        "  preds = torch.cat(preds, 0)\n",
        "  targets = torch.cat(targets, 0)\n",
        "  \n",
        "  acc = accuracy(preds, targets)\n",
        "  auc_v = auc(preds, targets, reorder=True)\n",
        "  pre, rec = precision_recall(preds, targets)\n",
        "  score = f1_score(preds, targets)\n",
        "  print(f'V {epoch}: acc = {acc.item():.2f} auc = {auc_v.item():.2f} pre = {pre.item():.2f} rec = {rec.item():.2f} f1_score = {score.item():.2f}')\n",
        "  wandb.log({'accuracy': acc.item(), \n",
        "             'auc': auc_v.item(),\n",
        "             'precision': pre.item(),\n",
        "             'recall': rec.item(),\n",
        "             'f1 score': score.item()})\n",
        "  return score.item()\n"
      ],
      "metadata": {
        "id": "ZzrArDoEXO7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging setup and Training"
      ],
      "metadata": {
        "id": "o_n5Fx_eXSxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_of_run=\"rand\"\n",
        "wandb.init(project=\"asd-trans\", name=name_of_run)"
      ],
      "metadata": {
        "id": "82LFn0iIXPqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"gdrive/MyDrive/ASD/Transformer-Models\", exist_ok=True)"
      ],
      "metadata": {
        "id": "1pJlNYmoYMaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = 0\n",
        "best_f1, best_epoch = 0,0\n",
        "f1_s = validation(0)\n",
        "for epoch in range(num_epochs):\n",
        "    adjust_lr(optimizer,epoch)\n",
        "    \n",
        "    iteration = train(iteration)\n",
        "    f1_s = validation(epoch)\n",
        "  \n",
        "    if f1_s > best_f1:\n",
        "        torch.save(model.state_dict(),os.path.join(checkpoint_path,'best_model_epoch'+str(epoch)+'.pth'))\n",
        "        best_f1 = f1_s\n",
        "        best_epoch = epoch\n",
        "\n",
        "print(f'Best F1 score at epoch {best_epoch}: {f1_s}')\n",
        "dir_name = f'gdrive/MyDrive/ASD/Transformer-Models/{name_of_run}/'\n",
        "src_file = f'{checkpoint_path}/best_model_epoch{best_epoch}.pth'\n",
        "dest = f'gdrive/MyDrive/ASD/Transformer-Models/{name_of_run}/best_model_epoch{best_epoch}.pth'\n",
        "os.makedirs(dir_name, exist_ok=True)\n",
        "shutil.move(src_file, dest)"
      ],
      "metadata": {
        "id": "iHDluF3sXkP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVXKhVZ1YU-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}