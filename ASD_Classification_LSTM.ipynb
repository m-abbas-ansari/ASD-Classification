{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbKxdF+/tpRn5E4krgEqPM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-abbas-ansari/ASD-Classification/blob/main/ASD_Classification_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Setup and Dataset Download"
      ],
      "metadata": {
        "id": "8I2_51BFTAua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('./gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggh8nBCNXogB",
        "outputId": "7ce8c9ee-1b94-4db1-f0a5-5229a0b47dce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd3LL4_xSvPI"
      },
      "outputs": [],
      "source": [
        "!pip -q install wget wandb torchmetrics\n",
        "import wget\n",
        "wget.download(\"https://zenodo.org/record/2647418/files/TrainingDataset.rar?download=1\")\n",
        "!mkdir Dataset\n",
        "!unrar x TrainingDataset.rar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Utilities"
      ],
      "metadata": {
        "id": "_rFg663JUhu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import operator\n",
        "from glob import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.functional import accuracy, auc, precision_recall, f1_score"
      ],
      "metadata": {
        "id": "f8OWmsakS8rv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(anno_path):\n",
        "    anno_dict = dict()\n",
        "    max_len = dict()\n",
        "    # Saliency4ASD has 300 images\n",
        "    for i in range(1,301):\n",
        "        img = cv2.imread(os.path.join(anno_path,'Images',str(i)+'.png'))\n",
        "        y_lim, x_lim, _ = img.shape\n",
        "        anno_dict[i] = dict()\n",
        "        anno_dict[i]['img_size'] = [y_lim,x_lim]\n",
        "        asd = pd.read_csv(os.path.join(anno_path,'ASD','ASD_scanpath_'+str(i)+'.txt'))\n",
        "        ctrl = pd.read_csv(os.path.join(anno_path,'TD','TD_scanpath_'+str(i)+'.txt'))\n",
        "        group_name = ['ctrl','asd']\n",
        "        for flag, group in enumerate([ctrl, asd]):\n",
        "            anno_dict[i][group_name[flag]] = dict()\n",
        "            anno_dict[i][group_name[flag]]['fixation'] = []\n",
        "            anno_dict[i][group_name[flag]]['duration'] = []\n",
        "            cur_idx = list(group['Idx'])\n",
        "            cur_x = list(group[' x'])\n",
        "            cur_y = list(group[' y'])\n",
        "            cur_dur = list(group[' duration'])\n",
        "            tmp_fix = []\n",
        "            tmp_dur = []\n",
        "            for j in range(len(cur_idx)):\n",
        "                # finish loading data for one subject\n",
        "                if cur_idx[j] == 0  and j != 0:\n",
        "                    anno_dict[i][group_name[flag]]['fixation'].append(tmp_fix)\n",
        "                    anno_dict[i][group_name[flag]]['duration'].append(tmp_dur)\n",
        "                    tmp_fix = []\n",
        "                    tmp_dur = []\n",
        "                tmp_fix.append([cur_y[j],cur_x[j]])\n",
        "                tmp_dur.append(cur_dur[j])\n",
        "            # save data of the last subject\n",
        "            anno_dict[i][group_name[flag]]['fixation'].append(tmp_fix)\n",
        "            anno_dict[i][group_name[flag]]['duration'].append(tmp_dur)\n",
        "\n",
        "    return anno_dict"
      ],
      "metadata": {
        "id": "26z1J70dUoOj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(data.Dataset):\n",
        "    def __init__(self,img_dir,data,max_len,img_height,img_width,transform):\n",
        "        self.img_dir = img_dir\n",
        "        self.initial_dataset(data)\n",
        "        self.max_len = max_len\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.transform = transform\n",
        "\n",
        "    def initial_dataset(self,data):\n",
        "        self.fixation = []\n",
        "        self.duration = []\n",
        "        self.label = []\n",
        "        self.img_id = []\n",
        "        self.img_size = []\n",
        "\n",
        "        for img_id in data.keys():\n",
        "            # if not img_id in valid_id:\n",
        "            #     continue\n",
        "            for group_label, group in enumerate(['ctrl','asd']):\n",
        "                self.fixation.extend(data[img_id][group]['fixation'])\n",
        "                self.duration.extend(data[img_id][group]['duration'])\n",
        "                self.img_id.extend([os.path.join(self.img_dir,str(img_id)+'.png')]*len(data[img_id][group]['fixation']))\n",
        "                self.label.extend([group_label]*len(data[img_id][group]['fixation']))\n",
        "                self.img_size.extend([data[img_id]['img_size']]*len(data[img_id][group]['fixation']))\n",
        "\n",
        "    def get_fix_dur(self,idx):\n",
        "        fixs = self.fixation[idx]\n",
        "        durs = self.duration[idx]\n",
        "        y_lim, x_lim = self.img_size[idx]\n",
        "        fixation = []\n",
        "        duration = []\n",
        "        invalid = 0\n",
        "        # only consider the first k fixations\n",
        "        for i in range(self.max_len):\n",
        "            if i+1 <= len(fixs):\n",
        "                y_fix, x_fix = fixs[i]\n",
        "                dur = durs[i]\n",
        "                x_fix = int(x_fix*(self.img_width/float(x_lim))/32)\n",
        "                y_fix = int(y_fix*(self.img_height/float(y_lim))/33)\n",
        "                if x_fix >=0 and y_fix>=0:\n",
        "                    fixation.append(y_fix*25 + x_fix) # get the corresponding index of fixation on the downsampled feature map\n",
        "                    duration.append(dur) # duration of corresponding fixation\n",
        "                else:\n",
        "                    invalid += 1\n",
        "            else:\n",
        "                fixation.append(0) # pad if necessary\n",
        "                duration.append(0)\n",
        "        for i in range(invalid):\n",
        "            fixation.append(0)\n",
        "            duration.append(0)\n",
        "        fixation = torch.from_numpy(np.array(fixation).astype('int'))\n",
        "        duration = torch.from_numpy(np.array(duration).astype('int'))\n",
        "        return fixation, duration\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img = Image.open(self.img_id[index])\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = torch.FloatTensor([self.label[index]])\n",
        "        fixation, duration = self.get_fix_dur(index)\n",
        "        return img, label, fixation, duration\n",
        "\n",
        "    def __len__(self,):\n",
        "        return len(self.fixation)"
      ],
      "metadata": {
        "id": "iDX0XnN3UqF-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def simple_img_split(anno_dict, val_ratio):\n",
        "  num_imgs = len(anno_dict.keys())\n",
        "  print(num_imgs)\n",
        "  val_idx = list(random.sample(range(1, num_imgs+1), k=int(val_ratio*num_imgs))) # randomly select validation images\n",
        "  train_idx = list(set(val_idx) ^ set(range(1, num_imgs+1)))\n",
        "  train_dict = {k: anno_dict[k] for k in train_idx}\n",
        "  val_dict = {k: anno_dict[k] for k in val_idx}\n",
        "\n",
        "  return train_dict, val_dict\n",
        "\n",
        "os.makedirs('checkpoints', exist_ok=True)"
      ],
      "metadata": {
        "id": "b0Xtlf-bUr8C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Archtitecture"
      ],
      "metadata": {
        "id": "H5GOGWwvUwLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "def adjust_lr(optimizer, epoch):\n",
        "    \"adatively adjust lr based on epoch\"\n",
        "    if epoch <= 0 :\n",
        "        lr = LR\n",
        "    else :\n",
        "        lr = LR * (0.5 ** (float(epoch) / 2))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "MeMGc45aUt5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class G_LSTM(nn.Module):\n",
        "\t\"\"\"\n",
        "\tLSTM implementation proposed by A. Graves (2013),\n",
        "\tit has more parameters compared to original LSTM\n",
        "\t\"\"\"\n",
        "\tdef __init__(self,input_size=2048,hidden_size=512):\n",
        "\t\tsuper(G_LSTM,self).__init__()\n",
        "\t\t# without batch_norm\n",
        "\t\tself.input_x = nn.Linear(input_size,hidden_size,bias=True)\n",
        "\t\tself.forget_x = nn.Linear(input_size,hidden_size,bias=True)\n",
        "\t\tself.output_x = nn.Linear(input_size,hidden_size,bias=True)\n",
        "\t\tself.memory_x = nn.Linear(input_size,hidden_size,bias=True)\n",
        "\n",
        "\t\tself.input_h = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\t\tself.forget_h = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\t\tself.output_h = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\t\tself.memory_h = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\n",
        "\t\tself.input_c = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\t\tself.forget_c = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\t\tself.output_c = nn.Linear(hidden_size,hidden_size,bias=True)\n",
        "\n",
        "\tdef forward(self,x,state):\n",
        "\t\th, c = state\n",
        "\t\ti = torch.sigmoid(self.input_x(x) + self.input_h(h) + self.input_c(c))\n",
        "\t\tf = torch.sigmoid(self.forget_x(x) + self.forget_h(h) + self.forget_c(c))\n",
        "\t\tg = torch.tanh(self.memory_x(x) + self.memory_h(h))\n",
        "\n",
        "\t\tnext_c = torch.mul(f,c) + torch.mul(i,g)\n",
        "\t\to = torch.sigmoid(self.output_x(x) + self.output_h(h) + self.output_c(next_c))\n",
        "\t\th = torch.mul(o,next_c)\n",
        "\t\tstate = (h,next_c)\n",
        "\n",
        "\t\treturn state"
      ],
      "metadata": {
        "id": "0mNb2ZIZUzdX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code from https://github.com/szzexpoi/attention_asd_screening/blob/master/model/model.py\n",
        "# Code was modified to include time-dependent representation techniques \n",
        "# i.e time-masking and time-event joint embedding\n",
        "\n",
        "class Sal_seq(nn.Module):\n",
        "    def __init__(self, backend, seq_len, all_lstm=False, crop_seq=False, mask=False, joint=False, time_proj_dim=128, hidden_size=512):\n",
        "        super(Sal_seq,self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.mask = mask\n",
        "        self.joint = joint\n",
        "        self.crop = crop_seq\n",
        "        self.all = all_lstm\n",
        "        # defining backend\n",
        "        if backend == 'resnet':\n",
        "            resnet = models.resnet50(pretrained=True)\n",
        "            self.init_resnet(resnet)\n",
        "            input_size = 2048\n",
        "        elif backend == 'vgg':\n",
        "            vgg = models.vgg19(pretrained=True)\n",
        "            self.init_vgg(vgg)\n",
        "            input_size = 512\n",
        "        else:\n",
        "            assert 0, 'Backend not implemented'\n",
        "        \n",
        "        self.rnn = G_LSTM(input_size,hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size,1,bias=True) # comment for multi-modal distillation\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.all:\n",
        "          self.predecode = nn.Linear(hidden_size*seq_len, hidden_size, bias=True)\n",
        "\n",
        "        if self.mask or self.joint:\n",
        "          self.time_projection = nn.Linear(1, time_proj_dim) # project duration to a time emedding space\n",
        "          if self.mask:\n",
        "            self.project_emb = nn.Linear(time_proj_dim, input_size) # take time embedding to the same emedding as input feature\n",
        "          if self.joint:\n",
        "            self.time_emb = nn.Linear(time_proj_dim, input_size, bias=False) # time embedding matrix \n",
        "\n",
        "    def init_resnet(self,resnet):\n",
        "        self.backend = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "    def init_vgg(self,vgg):\n",
        "        # self.backend = vgg.features\n",
        "        self.backend = nn.Sequential(*list(vgg.features.children())[:-2]) # omitting the last Max Pooling\n",
        "\n",
        "    def init_hidden(self,batch): #initializing hidden state as all zero\n",
        "        h = torch.zeros(batch,self.hidden_size).cuda()\n",
        "        c = torch.zeros(batch,self.hidden_size).cuda()\n",
        "        return (Variable(h),Variable(c))\n",
        "\n",
        "    def process_lengths(self,input):\n",
        "        \"\"\"\n",
        "        Computing the lengths of sentences in current batchs\n",
        "        \"\"\"\n",
        "        max_length = input.size(1)\n",
        "        lengths = list(max_length - input.data.eq(0).sum(1).squeeze())\n",
        "        return lengths\n",
        "\n",
        "    def crop_seq(self,x,lengths):\n",
        "        \"\"\"\n",
        "        Adaptively select the hidden state at the end of sentences\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        seq_length = x.size(1)\n",
        "        mask = x.data.new().resize_as_(x.data).fill_(0)\n",
        "        for i in range(batch_size):\n",
        "            mask[i][lengths[i]-1].fill_(1)\n",
        "        mask = Variable(mask)\n",
        "        x = x.mul(mask)\n",
        "        x = x.sum(1).view(batch_size, x.size(2))\n",
        "        return x\n",
        "\n",
        "    def forward(self,x,fixation, duration):\n",
        "        valid_len = self.process_lengths(fixation) # computing valid fixation lengths\n",
        "        x = self.backend(x) # [12, 2048, 19, 25]\n",
        "        batch, feat, h, w = x.size()\n",
        "        x = x.view(batch,feat,-1) # [12, 2048, 475]\n",
        "        \n",
        "        # recurrent loop\n",
        "        state = self.init_hidden(batch) # initialize hidden state\n",
        "        fixation = fixation.view(fixation.size(0),1,fixation.size(1)) # [12, 1, 14]\n",
        "        fixation = fixation.expand(fixation.size(0),feat,fixation.size(2)) # [12, 2048, 14]\n",
        "        #print(f'x before gather: {x}')\n",
        "        x = x.gather(2,fixation)\n",
        "        output = []\n",
        "        for i in range(self.seq_len):\n",
        "            # extract features corresponding to current fixation\n",
        "            cur_x = x[:,:,i].contiguous()\n",
        "            if self.mask or self.joint:\n",
        "              cur_t = duration[:, i].contiguous().unsqueeze(1)\n",
        "              cur_t_proj = self.time_projection(cur_t)\n",
        "              \n",
        "              if self.joint: # time-event joint embedding\n",
        "                cur_t_enc = torch.softmax(cur_t_proj, dim=1)\n",
        "                cur_t_emb = self.time_emb(cur_t_enc)\n",
        "                cur_x = (cur_x + cur_t_emb)/2.0\n",
        "\n",
        "              if self.mask: # time mask\n",
        "                cur_t_proj = torch.relu(cur_t_proj)\n",
        "                cur_t_proj = self.project_emb(cur_t_proj)\n",
        "                time_mask = torch.sigmoid(cur_t_proj)\n",
        "                cur_x = torch.mul(cur_x, time_mask)\n",
        "\n",
        "            #LSTM forward\n",
        "            state = self.rnn(cur_x,state)\n",
        "            out = state[0].view(batch,1,self.hidden_size)\n",
        "            output.append(out)\n",
        "        \n",
        "        # selecting hidden states from the valid fixations without padding\n",
        "        output = torch.cat(output, 1) # [12, 14, 512]\n",
        "        if self.all:\n",
        "          seq_len = output.size(1)\n",
        "          output = self.predecode(output.view(batch, self.hidden_size*seq_len))\n",
        "        else:\n",
        "          if self.crop:\n",
        "            output = self.crop_seq(output,valid_len)\n",
        "          else:\n",
        "            output = output[:,-1,:].view(batch, self.hidden_size) # select the last output state of LSTM\n",
        "        output = torch.sigmoid(self.decoder(output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "tW1ZA4juU5OT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "SVzUWSvKVXjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-4\n",
        "img_dir = 'Dataset/TrainingData/Images'\n",
        "anno_dir = 'Dataset/TrainingData'\n",
        "backend = 'resnet'\n",
        "checkpoint_path= 'checkpoints'\n",
        "num_epochs = 10\n",
        "val_ratio = 0.1\n",
        "batch_size = 12\n",
        "max_len = 14\n",
        "clip = 10\n",
        "img_height = 600\n",
        "img_width = 800\n",
        "\n",
        "\n",
        "#Sal_seq\n",
        "hidden_size = 512\n",
        "mask = False        # flag to whether or not to create a time mask to endcode duration into fixations\n",
        "joint = False       # flag to whether or not to create a time-event joint embedding\n",
        "crop_seq = True\n",
        "all_lstm = False\n",
        "time_proj_dim = 256"
      ],
      "metadata": {
        "id": "7yRweKHGVJSC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Model Intialization"
      ],
      "metadata": {
        "id": "oIQIi6wSWJg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anno = read_dataset(anno_dir)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((img_height,img_width)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data, val_data = simple_img_split(anno, val_ratio)\n",
        "\n",
        "train_set = Dataset(img_dir, train_data, max_len, img_height, img_width, transform)\n",
        "val_set = Dataset(img_dir, val_data, max_len, img_height, img_width, transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUf_JauhVr6I",
        "outputId": "b07de0af-427e-4894-927a-d9c6c64624a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if mask or joint:\n",
        "  model = Sal_seq(backend=backend,seq_len=max_len,all_lstm=all_lstm,crop_seq=crop_seq, mask=mask,joint=joint,time_proj_dim=time_proj_dim,hidden_size=hidden_size)  \n",
        "else:\n",
        "  model = Sal_seq(backend=backend,seq_len=max_len,all_lstm=all_lstm,crop_seq=crop_seq, hidden_size=hidden_size)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-5) "
      ],
      "metadata": {
        "id": "aAdIoBwjWRRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Total parameters in model: {total_params:,}\")"
      ],
      "metadata": {
        "id": "u5H4KIiqWYTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Validation Loop"
      ],
      "metadata": {
        "id": "7ZxGLKstXJoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(iteration):\n",
        "    avg_loss = 0\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for j, (img,target,fix, dur) in enumerate(tqdm(trainloader)):\n",
        "        if len(img) < batch_size:\n",
        "            continue\n",
        "        img, target, fix, dur = Variable(img), Variable(target.type(torch.FloatTensor)), Variable(fix,requires_grad=False), Variable(dur.type(torch.FloatTensor), requires_grad=False)\n",
        "        if torch.cuda.is_available():\n",
        "          img, target, fix, dur = img.cuda(), target.cuda(), fix.cuda(), dur.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img,fix, dur)\n",
        "        loss = F.binary_cross_entropy(pred,target)\n",
        "        loss.backward()\n",
        "        if clip != -1:\n",
        "            clip_gradient(optimizer,clip)\n",
        "        optimizer.step()\n",
        "        avg_loss = (avg_loss*np.maximum(0,j) + loss.data.cpu().numpy())/(j+1)\n",
        "\n",
        "        if j%25 == 0:\n",
        "            wandb.log({'bce loss': avg_loss}, step=iteration)\n",
        "        iteration += 1\n",
        "\n",
        "        preds.append(pred.cpu())\n",
        "        targets.append(target.to(torch.int16).cpu())\n",
        "    with torch.no_grad():\n",
        "      preds = torch.cat(preds, 0)\n",
        "      targets = torch.cat(targets, 0)\n",
        "      acc = accuracy(preds, targets)\n",
        "      auc_v = auc(preds, targets, reorder=True)\n",
        "      pre, rec = precision_recall(preds, targets)\n",
        "      score = f1_score(preds, targets)\n",
        "      print(f'\\nT {epoch}: acc = {acc.item():.2f} auc = {auc_v.item():.2f} pre = {pre.item():.2f} rec = {rec.item():.2f} f1_score = {score.item():.2f}')\n",
        "\n",
        "    return iteration"
      ],
      "metadata": {
        "id": "cvde6T1UW6LK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(epoch):\n",
        "  \n",
        "  preds = []\n",
        "  targets = []\n",
        "  with torch.no_grad():\n",
        "    for _, (img,target,fix, dur) in enumerate(valloader):\n",
        "      img, target, fix, dur = Variable(img), Variable(target.type(torch.FloatTensor)), Variable(fix,requires_grad=False), Variable(dur.type(torch.FloatTensor), requires_grad=False)\n",
        "      img, fix, dur = img.cuda(), fix.cuda(), dur.cuda()\n",
        "      #print(f'img: {img.shape} target: {target} fix: {fix.shape}')\n",
        "      # break\n",
        "      pred = model(img,fix, dur)\n",
        "      preds.append(pred.cpu())\n",
        "      targets.append(target.to(torch.int16))\n",
        "\n",
        "  preds = torch.cat(preds, 0)\n",
        "  targets = torch.cat(targets, 0)\n",
        "  \n",
        "  acc = accuracy(preds, targets)\n",
        "  auc_v = auc(preds, targets, reorder=True)\n",
        "  pre, rec = precision_recall(preds, targets)\n",
        "  score = f1_score(preds, targets)\n",
        "  print(f'V {epoch}: acc = {acc.item():.2f} auc = {auc_v.item():.2f} pre = {pre.item():.2f} rec = {rec.item():.2f} f1_score = {score.item():.2f}')\n",
        "  wandb.log({'accuracy': acc.item(), \n",
        "             'auc': auc_v.item(),\n",
        "             'precision': pre.item(),\n",
        "             'recall': rec.item(),\n",
        "             'f1 score': score.item()})\n",
        "  return score.item()\n"
      ],
      "metadata": {
        "id": "ZzrArDoEXO7U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging setup and Training"
      ],
      "metadata": {
        "id": "o_n5Fx_eXSxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_of_run=\"rand\"\n",
        "wandb.init(project=\"asd-lstm\", name=name_of_run)"
      ],
      "metadata": {
        "id": "82LFn0iIXPqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"gdrive/MyDrive/ASD/LSTM-Models\", exist_ok=True)"
      ],
      "metadata": {
        "id": "1pJlNYmoYMaN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = 0\n",
        "best_f1, best_epoch = 0,0\n",
        "f1_s = validation(0)\n",
        "for epoch in range(num_epochs):\n",
        "    adjust_lr(optimizer,epoch)\n",
        "    \n",
        "    iteration = train(iteration)\n",
        "    f1_s = validation(epoch)\n",
        "  \n",
        "    if f1_s > best_f1:\n",
        "        torch.save(model.state_dict(),os.path.join(checkpoint_path,'best_model_epoch'+str(epoch)+'.pth'))\n",
        "        best_f1 = f1_s\n",
        "        best_epoch = epoch\n",
        "\n",
        "print(f'Best F1 score at epoch {best_epoch}: {f1_s}')\n",
        "dir_name = f'gdrive/MyDrive/ASD/LSTM-Models/{name_of_run}/'\n",
        "src_file = f'{checkpoint_path}/best_model_epoch{best_epoch}.pth'\n",
        "dest = f'gdrive/MyDrive/ASD/LSTM-Models/{name_of_run}/best_model_epoch{best_epoch}.pth'\n",
        "os.makedirs(dir_name, exist_ok=True)\n",
        "shutil.move(src_file, dest)"
      ],
      "metadata": {
        "id": "iHDluF3sXkP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVXKhVZ1YU-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}